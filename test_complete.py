#!/usr/bin/env python3
"""Complete test suite for Gemini MCP Python orchestrator"""

import os
import asyncio
import json
from datetime import datetime
from src.gemini_mcp.gemini_client import GeminiClient
from src.gemini_mcp.orchestrator import TaskOrchestrator

async def run_complete_test():
    """Run comprehensive test of all orchestrator functionality"""
    print("ğŸš€ Starting Complete Gemini MCP Orchestrator Test")
    print("=" * 60)
    
    # Load environment
    from dotenv import load_dotenv
    load_dotenv()
    
    results = {"tests": [], "start_time": datetime.now().isoformat()}
    
    try:
        # Initialize
        client = GeminiClient()
        orchestrator = TaskOrchestrator(client, max_concurrent=3)
        print("âœ… Orchestrator initialized successfully")
        
        # Test 1: Single prompt
        print("\nğŸ§ª Test 1: Single Prompt Execution")
        result1 = await orchestrator.quick_ask("What is 2+2? Respond with just the number.")
        print(f"Result: {result1}")
        results["tests"].append({"test": "single_prompt", "status": "passed", "result": result1})
        
        # Test 2: Parallel execution
        print("\nğŸ§ª Test 2: Parallel Prompt Execution")
        prompts = [
            "What is the capital of France?",
            "What color is the sun?", 
            "What is 10 x 10?"
        ]
        parallel_results = await orchestrator.parallel_ask(prompts)
        print(f"Parallel results: {len(parallel_results)} responses")
        for i, result in enumerate(parallel_results):
            print(f"  {i+1}: {result[:50]}...")
        results["tests"].append({"test": "parallel_execution", "status": "passed", "count": len(parallel_results)})
        
        # Test 3: File creation (multiple files)
        print("\nğŸ§ª Test 3: File Creation Tests")
        test_files = []
        
        # Create test files
        files_to_create = [
            {
                "path": "/home/arvind/desi-feynman/gemini-mcp-python/tests/test_math.py",
                "content": "def add(a, b): return a + b\ndef multiply(a, b): return a * b"
            },
            {
                "path": "/home/arvind/desi-feynman/gemini-mcp-python/tests/test_strings.py", 
                "content": "def reverse_string(s): return s[::-1]\ndef uppercase(s): return s.upper()"
            },
            {
                "path": "/home/arvind/desi-feynman/gemini-mcp-python/tests/test_data.json",
                "content": json.dumps({"test": "data", "timestamp": datetime.now().isoformat()}, indent=2)
            }
        ]
        
        for file_info in files_to_create:
            try:
                os.makedirs(os.path.dirname(file_info["path"]), exist_ok=True)
                with open(file_info["path"], 'w') as f:
                    f.write(file_info["content"])
                
                if os.path.exists(file_info["path"]):
                    size = os.path.getsize(file_info["path"])
                    print(f"âœ… Created: {os.path.basename(file_info['path'])} ({size} bytes)")
                    test_files.append(file_info["path"])
                else:
                    print(f"âŒ Failed: {os.path.basename(file_info['path'])}")
                    
            except Exception as e:
                print(f"âŒ Error creating {file_info['path']}: {e}")
        
        results["tests"].append({"test": "file_creation", "status": "passed", "files_created": len(test_files)})
        
        # Test 4: Code generation via Gemini
        print("\nğŸ§ª Test 4: Gemini Code Generation")
        code_prompt = "Create a simple Python class called Calculator with methods for add, subtract, multiply, and divide. Include docstrings."
        generated_code = await orchestrator.quick_ask(code_prompt)
        
        # Save generated code
        generated_file = "/home/arvind/desi-feynman/gemini-mcp-python/tests/generated_calculator.py"
        try:
            with open(generated_file, 'w') as f:
                f.write(f"# Generated by Gemini at {datetime.now()}\n\n{generated_code}")
            
            if os.path.exists(generated_file):
                size = os.path.getsize(generated_file)
                print(f"âœ… Generated code saved: {size} bytes")
                results["tests"].append({"test": "code_generation", "status": "passed", "file_size": size})
            else:
                print("âŒ Code generation file not created")
                results["tests"].append({"test": "code_generation", "status": "failed", "error": "file_not_created"})
                
        except Exception as e:
            print(f"âŒ Error saving generated code: {e}")
            results["tests"].append({"test": "code_generation", "status": "failed", "error": str(e)})
        
        # Test 5: Error handling
        print("\nğŸ§ª Test 5: Error Handling")
        try:
            # Test with empty prompt
            await orchestrator.quick_ask("")
            print("âŒ Should have failed with empty prompt")
            results["tests"].append({"test": "error_handling", "status": "failed", "reason": "no_error_on_empty_prompt"})
        except Exception as e:
            print(f"âœ… Correctly handled empty prompt error: {type(e).__name__}")
            results["tests"].append({"test": "error_handling", "status": "passed", "error_type": type(e).__name__})
        
        # Summary
        print("\nğŸ“Š Test Summary")
        print("=" * 40)
        passed = sum(1 for test in results["tests"] if test["status"] == "passed")
        total = len(results["tests"])
        print(f"Passed: {passed}/{total}")
        print(f"Success Rate: {(passed/total)*100:.1f}%")
        
        results["summary"] = {
            "total_tests": total,
            "passed": passed,
            "success_rate": (passed/total)*100
        }
        
        # Save results
        results_file = "/home/arvind/desi-feynman/gemini-mcp-python/test_results.json"
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"ğŸ“ Results saved to: {results_file}")
        
        if passed == total:
            print("\nğŸ‰ ALL TESTS PASSED! Orchestrator is fully functional!")
            return True
        else:
            print(f"\nâš ï¸  {total-passed} tests failed. Check results for details.")
            return False
            
    except Exception as e:
        print(f"\nâŒ Critical error in test suite: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = asyncio.run(run_complete_test())
    exit(0 if success else 1)